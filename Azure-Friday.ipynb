{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976c6b8a",
   "metadata": {},
   "source": [
    "# Making sense of the world through vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88150b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Custom Vision\n",
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateEntry, ImageFileCreateBatch\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "\n",
    "# Face API\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person\n",
    "\n",
    "# Speech API\n",
    "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
    "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Other\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from PIL import Image #, ImageDraw\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPythonImage\n",
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into our Azure Subscription\n",
    "# az login --use-device-code\n",
    "\n",
    "# Create a resource group\n",
    "! az group create -n Azure-Friday_RG -l westeurope --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az cognitiveservices account create \\\n",
    "    --name AF-ComputerVision \\\n",
    "    --resource-group Azure-Friday_RG \\\n",
    "    --kind ComputerVision \\\n",
    "    --sku S1 \\\n",
    "    --location westeurope \\\n",
    "    --yes \\\n",
    "    --output table\n",
    "\n",
    "! az cognitiveservices account keys list \\\n",
    "    --name AF-ComputerVision --resource-group Azure-Friday_RG \\\n",
    "    --query key1\n",
    "\n",
    "! az cognitiveservices account show \\\n",
    "    --name AF-ComputerVision --resource-group Azure-Friday_RG \\\n",
    "    --query properties.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f89d47",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subscription_key = \"<INSERT KEY>\"\n",
    "\n",
    "endpoint = \"https://westeurope.api.cognitive.microsoft.com/\" \n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97958ead",
   "metadata": {},
   "source": [
    "## Describe what is on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"images/amsterdam-gaa24fa0bd_1280.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPythonImage(filename=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244875da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    description_results = computervision_client.describe_image_in_stream(image_stream)\n",
    "\n",
    "    for description in description_results.captions:\n",
    "        print(description.text, \"| Confidence: \",\"%.2f\" % description.confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb1315",
   "metadata": {},
   "source": [
    "## Detect what is on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect objects in the Images\n",
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    detect_objects_results_remote = computervision_client.detect_objects_in_stream(image_stream)\n",
    "\n",
    "    im = plt.imread(image_url)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig = plt.figure(figsize = (im.shape[1]/80, im.shape[0]/80))\n",
    "    ax = plt.axes((0,0,1,1))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im,origin='upper')\n",
    "\n",
    "    # Overlay the information\n",
    "    for object in detect_objects_results_remote.objects:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((object.rectangle.x, object.rectangle.y), \n",
    "                                 object.rectangle.w, object.rectangle.h, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*object.rectangle.x), 1-(1/im.shape[0]*object.rectangle.y), object.object_property,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=16,\n",
    "            color='w',\n",
    "            backgroundcolor=color,\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd13b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_note_img = \"bank-notes/Test/2Thousandnote/3.jpg\"\n",
    "\n",
    "with open(os.path.join(bank_note_img), \"rb\") as image_stream:\n",
    "    \n",
    "    # detect objects in the image\n",
    "    detect_objects_results_remote = computervision_client.detect_objects_in_stream(image_stream)\n",
    "\n",
    "    im = plt.imread(bank_note_img)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "    ax = plt.axes((0,0,1,1))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im,origin='upper')\n",
    "\n",
    "    # Overlay the information\n",
    "    for object in detect_objects_results_remote.objects:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((object.rectangle.x, object.rectangle.y), \n",
    "                                 object.rectangle.w, object.rectangle.h, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*object.rectangle.x), 1-(1/im.shape[0]*object.rectangle.y), object.object_property,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=16,\n",
    "            color='w',\n",
    "            backgroundcolor=color,\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3d54a",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Detect your own objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85454bb1",
   "metadata": {},
   "source": [
    "## Train our own model using Azure Custom Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be41812",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az cognitiveservices account create \\\n",
    "    --name AF-CustomVision \\\n",
    "    --kind CustomVision.Training \\\n",
    "    --sku S0 \\\n",
    "    --resource-group Azure-Friday_RG \\\n",
    "    --location westeurope \\\n",
    "    --yes \\\n",
    "    --output table\n",
    "\n",
    "! az cognitiveservices account keys list \\\n",
    "    --name AF-CustomVision \\\n",
    "    --resource-group Azure-Friday_RG \\\n",
    "    --query key1\n",
    "    \n",
    "! az cognitiveservices account show  \\\n",
    "    --name AF-CustomVision  \\\n",
    "    --resource-group Azure-Friday_RG  \\\n",
    "    -o json  \\\n",
    "    --query properties.endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training credentials\n",
    "training_cv_key = \"<INSERT KEY>\"\n",
    "cv_endpoint = \"https://westeurope.api.cognitive.microsoft.com\"\n",
    "\n",
    "# Location for the training images\n",
    "training_images = \"bank-notes/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the training endpoint\n",
    "credentials = ApiKeyCredentials(in_headers={\"Training-key\": training_cv_key})\n",
    "trainer = CustomVisionTrainingClient(endpoint=cv_endpoint, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da528dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in trainer.get_domains():\n",
    "    print(domain.id, \"\\t\", domain.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new project using the standard domain\n",
    "project = trainer.create_project(\"Indian Bank Notes V1\", domain_id=\"0732100f-1a38-4e49-a514-c9b44c697ab5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "\n",
    "path = r\"bank-notes/train\"\n",
    "random_filenames = []\n",
    "for tag in os.listdir(path):\n",
    "    random_filenames.append(path+\"/\"+tag+\"/\"+random.choice([\n",
    "        x for x in os.listdir(os.path.join(path,tag))\n",
    "        if os.path.isfile(os.path.join(path,tag, x))\n",
    "    ]))\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(2, 4), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "for img_name in random_filenames[0:10]:\n",
    "    im = plt.imread(img_name)\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,1,0,0.8), alpha=1, origin='upper', zorder=-1)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42138efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload the images in batches\n",
    "image_list = []\n",
    "directories = os.listdir(training_images)\n",
    "\n",
    "for tagName in directories:\n",
    "    tag = trainer.create_tag(project.id, tagName)\n",
    "    images = os.listdir(os.path.join(training_images,tagName))\n",
    "    for img in images:\n",
    "        with open(os.path.join(training_images,tagName,img), \"rb\") as image_contents:\n",
    "            image_list.append(ImageFileCreateEntry(name=img, contents=image_contents.read(), tag_ids=[tag.id]))  \n",
    "            \n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "batchedImages = chunks(image_list, 64)\n",
    "\n",
    "for batchOfImages in batchedImages:\n",
    "    upload_result = trainer.create_images_from_files(project.id, ImageFileCreateBatch(images=batchOfImages))\n",
    "    if not upload_result.is_batch_successful:\n",
    "        print(\"Image batch upload failed.\")\n",
    "        for image in upload_result.images:\n",
    "            print(\"Image status: \", image.status)\n",
    "    else:\n",
    "        print(\"Batch uploaded successfully\")\n",
    "print(\"Done uploading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdefac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print (\"Start Training...\")\n",
    "iteration = trainer.train_project(project.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for completion\n",
    "while (iteration.status != \"Completed\"):\n",
    "    iteration = trainer.get_iteration(project.id, iteration.id)\n",
    "    print (\"Training status: \" + iteration.status)\n",
    "    print (\"Waiting 10 seconds...\")\n",
    "    time.sleep(10)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79add3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"ONNX\"\n",
    "flavor = \"ONNX12\"\n",
    "iteration_id =  iteration.id \n",
    "project_id =  project.id \n",
    "export = trainer.export_iteration(project_id, iteration_id , platform, flavor, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (export.status == \"Exporting\"):\n",
    "    print (\"Waiting 5 seconds...\")\n",
    "    time.sleep(5)\n",
    "    exports = trainer.get_exports(project.id, iteration_id)\n",
    "    # Locate the export for this iteration and check its status  \n",
    "    for e in exports:\n",
    "        if e.platform == export.platform and e.flavor == export.flavor:\n",
    "            export = e\n",
    "            break\n",
    "    print(\"Export status is: \", export.status)\n",
    "\n",
    "print(\"Export: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c379582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly download a previous export\n",
    "iteration_id =  \"11c31b66-6c9c-4b4f-98bf-84e73827a56d\" # iteration.id \n",
    "project_id =  \"e4165ef1-ab02-4909-a05d-2cf924125ccb\" # project.id \n",
    "\n",
    "platform = \"ONNX\"\n",
    "flavor = \"ONNX12\"\n",
    "\n",
    "exports = trainer.get_exports(project_id, iteration_id)\n",
    "for e in exports:\n",
    "    if e.platform == platform:\n",
    "        export = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec997bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# now we can download it\n",
    "export_file = requests.get(export.download_uri)\n",
    "with open(\"export.zip\", \"wb\") as file:\n",
    "    file.write(export_file.content)\n",
    "        \n",
    "# Unzip the downloaded export\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.mkdir(\"./model\");\n",
    "zip_ref = zipfile.ZipFile(\"export.zip\", 'r')\n",
    "zip_ref.extractall(\"./model\")\n",
    "zip_ref.close()\n",
    "print(\"Data extracted in: ./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as nxrun\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "training_images = \"./bank-notes/TestImages\"\n",
    "model_path = \"./model/model.onnx\"\n",
    "\n",
    "sess = nxrun.InferenceSession(model_path)\n",
    "\n",
    "testimages = os.listdir(training_images)\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(1, 4), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "for image_filepath in testimages[0:5]:\n",
    "    image = PIL.Image.open(os.path.join(training_images,image_filepath)).resize([224,224])\n",
    "    input_array = np.array(image, dtype=np.float32)[np.newaxis, :, :, :]\n",
    "    input_array = input_array.transpose((0, 3, 1, 2))[:, (2, 1, 0), :, :]\n",
    "\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    outputs = sess.run(None, {input_name: input_array.astype(np.float32)})\n",
    "    \n",
    "    im = plt.imread(os.path.join(training_images,image_filepath))\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,1,0,0.8), alpha=1, origin='upper', zorder=-1)\n",
    "    \n",
    "    grid[i].set_title(outputs[0][0][0], fontdict=None, loc='center', color = \"k\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b20054",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get more insights on Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8dc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az cognitiveservices account create \\\n",
    "    --name AF-Face \\\n",
    "    --resource-group Azure-Friday_RG \\\n",
    "    --kind Face \\\n",
    "    --sku S0 \\\n",
    "    --location westeurope \\\n",
    "    --yes \\\n",
    "    --output table\n",
    "\n",
    "! az cognitiveservices account keys list \\\n",
    "    --name AF-Face --resource-group Azure-Friday_RG \\\n",
    "    --query key1\n",
    "\n",
    "! az cognitiveservices account show \\\n",
    "    --name AF-Face --resource-group Azure-Friday_RG \\\n",
    "    --query properties.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef92a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_key = \"<INSERT KEY>\"\n",
    "face_api_endpoint = \"https://westeurope.api.cognitive.microsoft.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad810844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated FaceClient.\n",
    "face_client = FaceClient(face_api_endpoint, CognitiveServicesCredentials(face_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_image = \"face-photos/tech-a11y-crew.jpg\"\n",
    "display(IPythonImage(filename=mf_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66801a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    \n",
    "    # Detect faces in images\n",
    "    detected_faces = face_client.face.detect_with_stream(image_stream, return_face_attributes=[\n",
    "                    'age',  # Could have been the string 'age'\n",
    "                    'gender',\n",
    "                    'smile',\n",
    "                    'facialHair',\n",
    "                    'glasses',\n",
    "                    'emotion',\n",
    "                    'hair',\n",
    "                    'makeup',\n",
    "                    'accessories'\n",
    "                ])\n",
    "    \n",
    "    # Display the result\n",
    "    pil_img = Image.open(mf_image)\n",
    "    for face in detected_faces: \n",
    "        img2 = pil_img.crop((face.face_rectangle.left, face.face_rectangle.top, face.face_rectangle.left+face.face_rectangle.width, face.face_rectangle.top+face.face_rectangle.height))\n",
    "        display(img2)\n",
    "        print (f'Face id: {face.face_id}')\n",
    "        print (f'Gender: {face.face_attributes.gender}')\n",
    "        print (f'smile: {face.face_attributes.smile}')\n",
    "        print (f'age: {face.face_attributes.age}')\n",
    "        print (f'facial_hair moustache: {face.face_attributes.facial_hair.moustache}')\n",
    "        print (f'facial_hair beard: {face.face_attributes.facial_hair.beard}')\n",
    "        print (f'facial_hair sideburns: {face.face_attributes.facial_hair.sideburns}')\n",
    "        print (f'glasses: {face.face_attributes.glasses}')\n",
    "        print (f'eye_makeup: {face.face_attributes.makeup.eye_makeup}')\n",
    "        print (f'lip_makeup: {face.face_attributes.makeup.lip_makeup}')\n",
    "        print (f'emotion: {face.face_attributes.emotion}')\n",
    "        print(\" ==\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# Save this ID for use in Find Similar\n",
    "first_image_face_ID = detected_faces[0].face_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24188115",
   "metadata": {},
   "source": [
    "## Train the face API to recognize people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "path = r\"face-photos/train\"\n",
    "random_filenames = []\n",
    "for train_img in os.listdir(path):\n",
    "    random_filenames.append(os.path.join(path, train_img))\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(1, 5), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "for img_name in random_filenames[0:10]:\n",
    "    im = plt.imread(img_name)\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,0.8,0,1), alpha=1, origin='upper', zorder=-1)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_GROUP_ID = \"tech-a11y-crew\"\n",
    "face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=PERSON_GROUP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"face-photos/train\"\n",
    "\n",
    "for person in os.listdir(path):\n",
    "    name = person.partition(\".\")[0]\n",
    "    print(\"Adding:\"+name)\n",
    "    w = open(os.path.join(path,person), 'r+b')\n",
    "\n",
    "    # Create a person\n",
    "    person = face_client.person_group_person.create(PERSON_GROUP_ID, name)\n",
    "\n",
    "    # Add a face to the person\n",
    "    face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, person.person_id, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the person group\n",
    "face_client.person_group.train(PERSON_GROUP_ID)\n",
    "\n",
    "while (True):\n",
    "    training_status = face_client.person_group.get_training_status(PERSON_GROUP_ID)\n",
    "    print(\"Training status: {}.\".format(training_status.status))\n",
    "    if (training_status.status is TrainingStatusType.succeeded):\n",
    "        break\n",
    "    elif (training_status.status is TrainingStatusType.failed):\n",
    "        face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "        sys.exit('Training the person group has failed.')\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7315576",
   "metadata": {},
   "source": [
    "## Identify people in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    # Detect faces\n",
    "    face_ids = []\n",
    "    # We use detection model 3 to get better performance.\n",
    "    faces = face_client.face.detect_with_stream(image_stream, detection_model='detection_03')\n",
    "    for face in faces:\n",
    "        face_ids.append(face.face_id)\n",
    "        print(f'found face: {face.face_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44506a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify faces\n",
    "results = face_client.face.identify(face_ids, PERSON_GROUP_ID)\n",
    "\n",
    "identified_persons = {}\n",
    "\n",
    "for person in results:\n",
    "    for candidate in person.candidates:\n",
    "        identified_person = face_client.person_group_person.get(PERSON_GROUP_ID,candidate.person_id)\n",
    "        print(\"Found: \"+identified_person.name)\n",
    "        identified_persons[person.face_id] = identified_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "\n",
    "im = plt.imread(mf_image)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/70, im.shape[0]/70))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "# Overlay the information\n",
    "for face in faces:\n",
    "    color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "    rect = patches.Rectangle((face.face_rectangle.left, face.face_rectangle.top), \n",
    "                             face.face_rectangle.width, face.face_rectangle.height, \n",
    "                             linewidth=3, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    if face.face_id in identified_persons:\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*face.face_rectangle.left), 1-(1/im.shape[0]*face.face_rectangle.top), \n",
    "            \"{}\".format(identified_persons[face.face_id].name),\n",
    "            horizontalalignment='left', verticalalignment='bottom', fontsize=16, color='w', backgroundcolor=color, transform=ax.transAxes\n",
    "        )\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94dc3",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Read text in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee80930",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwriting_image_url = \"images/handwriting.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPythonImage(filename=handwriting_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Start =====\")\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "with open(os.path.join(handwriting_image_url), \"rb\") as image_stream:\n",
    "    read_response = computervision_client.read_in_stream(image_stream,  raw=True)\n",
    "\n",
    "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results \n",
    "while True:\n",
    "    read_result = computervision_client.get_read_result(operation_id)\n",
    "    if read_result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"===== Done =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(handwriting_image_url)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "full_text = \"\"\n",
    "for text_result in read_result.analyze_result.read_results:\n",
    "    for line in text_result.lines:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((line.bounding_box[0], line.bounding_box[1]), \n",
    "                             line.bounding_box[2]-line.bounding_box[0], line.bounding_box[5]-line.bounding_box[1], \n",
    "                             linewidth=6, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        #print(line.text)\n",
    "        full_text+=line.text + \" \"\n",
    "    \n",
    "print(full_text)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc04de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az cognitiveservices account create \\\n",
    "    --name AF-Speech \\\n",
    "    --resource-group Azure-Friday_RG \\\n",
    "    --kind SpeechServices \\\n",
    "    --sku S0 \\\n",
    "    --location westeurope \\\n",
    "    --yes \\\n",
    "    --output table\n",
    "\n",
    "! az cognitiveservices account keys list \\\n",
    "    --name AF-Speech --resource-group Azure-Friday_RG \\\n",
    "    --query key1\n",
    "\n",
    "! az cognitiveservices account show \\\n",
    "    --name AF-Speech --resource-group Azure-Friday_RG \\\n",
    "    --query properties.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa29626",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = \"<INSERT KEY>\"\n",
    "service_region = \"westeurope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config.speech_synthesis_language = \"en-GB\" \n",
    "speech_config.speech_synthesis_voice_name =\"en-GB-LibbyNeural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = full_text\n",
    "\n",
    "audio_file=f'{speech_config.speech_synthesis_voice_name}.wav'\n",
    "audio_config = AudioOutputConfig(filename=audio_file)\n",
    "synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "synthesizer.speak_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPythonAudio(audio_file,autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
