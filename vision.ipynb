{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976c6b8a",
   "metadata": {},
   "source": [
    "# Making sense of the world through vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88150b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import IPython\n",
    "#from IPython.display import Image \n",
    "from array import array\n",
    "#from PIL import Image as pilImage\n",
    "from skimage import io\n",
    "from io import BytesIO\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPythonImage\n",
    "\n",
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateEntry, ImageFileCreateBatch\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "\n",
    "import asyncio\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from io import BytesIO\n",
    "# To install this module, run:\n",
    "# python -m pip install Pillow\n",
    "from PIL import Image, ImageDraw\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e22be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_key = \"<INSERT KEY>\"\n",
    "endpoint = \"https://westeurope.api.cognitive.microsoft.com/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f89d47",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97958ead",
   "metadata": {},
   "source": [
    "## Describe what is on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"images/amsterdam-streets-2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPythonImage(filename=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244875da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    description_results = computervision_client.describe_image_in_stream(image_stream)\n",
    "\n",
    "    for description in description_results.captions:\n",
    "        print(description.text, \"| Confidence: \",\"%.2f\" % description.confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb1315",
   "metadata": {},
   "source": [
    "## Detect what is on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect objects in the Images\n",
    "with open(os.path.join(image_url), \"rb\") as image_stream:\n",
    "    detect_objects_results_remote = computervision_client.detect_objects_in_stream(image_stream)\n",
    "\n",
    "    im = plt.imread(image_url)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig = plt.figure(figsize = (im.shape[1]/80, im.shape[0]/80))\n",
    "    ax = plt.axes((0,0,1,1))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im,origin='upper')\n",
    "\n",
    "    # Overlay the information\n",
    "    for object in detect_objects_results_remote.objects:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((object.rectangle.x, object.rectangle.y), \n",
    "                                 object.rectangle.w, object.rectangle.h, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*object.rectangle.x), 1-(1/im.shape[0]*object.rectangle.y), object.object_property,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=16,\n",
    "            color='w',\n",
    "            backgroundcolor=color,\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd13b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_note_img = \"bank-notes/Test/2Thousandnote/3.jpg\"\n",
    "\n",
    "with open(os.path.join(bank_note_img), \"rb\") as image_stream:\n",
    "    \n",
    "    # detect objects in the image\n",
    "    detect_objects_results_remote = computervision_client.detect_objects_in_stream(image_stream)\n",
    "\n",
    "    im = plt.imread(bank_note_img)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "    ax = plt.axes((0,0,1,1))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im,origin='upper')\n",
    "\n",
    "    # Overlay the information\n",
    "    for object in detect_objects_results_remote.objects:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((object.rectangle.x, object.rectangle.y), \n",
    "                                 object.rectangle.w, object.rectangle.h, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            (1/im.shape[1]*object.rectangle.x), 1-(1/im.shape[0]*object.rectangle.y), object.object_property,\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=16,\n",
    "            color='w',\n",
    "            backgroundcolor=color,\n",
    "            transform=ax.transAxes\n",
    "        )\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3d54a",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Detect your own objects\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train our own model using Azure Custom Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training credentials\n",
    "training_key = \"<INSERT KEY>\"\n",
    "cv_endpoint = \"https://westeurope.api.cognitive.microsoft.com\"\n",
    "\n",
    "# Prediction credentials\n",
    "prediction_key = \"<INSERT KEY>\"\n",
    "prediction_resource_id = \"/subscriptions/431dbae5-40ca-438a-8daf-77d7d5580b41/resourceGroups/CustomVision_Demo-rg/providers/Microsoft.CognitiveServices/accounts/CustomVisionDemo-Prediction\"\n",
    "\n",
    "# Location for the training images\n",
    "training_images = \"bank-notes/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the training endpoint\n",
    "credentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\n",
    "trainer = CustomVisionTrainingClient(endpoint=cv_endpoint,credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da528dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in trainer.get_domains():\n",
    "    print(domain.id, \"\\t\", domain.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new project using the standard domain\n",
    "project = trainer.create_project(\"Bank Notes - 2\", domain_id=\"ee85a74c-405e-4adc-bb47-ffa8ca0c9f31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42138efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = []\n",
    "directories = os.listdir(training_images)\n",
    "\n",
    "for tagName in directories:\n",
    "    tag = trainer.create_tag(project.id, tagName)\n",
    "    images = os.listdir(os.path.join(training_images,tagName))\n",
    "    for img in images:\n",
    "        with open(os.path.join(training_images,tagName,img), \"rb\") as image_contents:\n",
    "            image_list.append(ImageFileCreateEntry(name=img, contents=image_contents.read(), tag_ids=[tag.id]))  \n",
    "            \n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "batchedImages = chunks(image_list, 64)\n",
    "\n",
    "for batchOfImages in batchedImages:\n",
    "    upload_result = trainer.create_images_from_files(project.id, ImageFileCreateBatch(images=batchOfImages))\n",
    "    if not upload_result.is_batch_successful:\n",
    "        print(\"Image batch upload failed.\")\n",
    "        for image in upload_result.images:\n",
    "            print(\"Image status: \", image.status)\n",
    "    else:\n",
    "        print(\"Batch uploaded successfully\")\n",
    "print(\"Done uploading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "path = r\"bank-notes/train\"\n",
    "random_filenames = []\n",
    "for tag in os.listdir(path):\n",
    "    random_filenames.append(path+\"/\"+tag+\"/\"+random.choice([\n",
    "        x for x in os.listdir(os.path.join(path,tag))\n",
    "        if os.path.isfile(os.path.join(path,tag, x))\n",
    "    ]))\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(2, 4), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "for img_name in random_filenames[0:10]:\n",
    "    im = plt.imread(img_name)\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,1,0,0.8), alpha=1, origin='upper', zorder=-1)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdefac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print (\"Start Training...\")\n",
    "iteration = trainer.train_project(project.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for completion\n",
    "while (iteration.status != \"Completed\"):\n",
    "    iteration = trainer.get_iteration(project.id, iteration.id)\n",
    "    print (\"Training status: \" + iteration.status)\n",
    "    print (\"Waiting 30 seconds...\")\n",
    "    time.sleep(30)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb000083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the model\n",
    "publish_iteration_name = \"version1\"\n",
    "\n",
    "print(\"Publishing the model\")\n",
    "trainer.publish_iteration(project.id, iteration.id, publish_iteration_name, prediction_resource_id)\n",
    "print (\"Publishing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there is a trained endpoint that can be used to make a prediction\n",
    "prediction_credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "predictor = CustomVisionPredictionClient(cv_endpoint, prediction_credentials)\n",
    "\n",
    "# Pick random images from the test set\n",
    "path = r\"bank-notes/Test\"\n",
    "random_filenames = []\n",
    "for tag in os.listdir(path):\n",
    "    random_filenames.append(path+\"/\"+tag+\"/\"+random.choice([\n",
    "        x for x in os.listdir(os.path.join(path,tag))\n",
    "        if os.path.isfile(os.path.join(path,tag, x))\n",
    "    ]))\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(4, 2), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "# Send the images one by one to the model and show the result\n",
    "i = 0\n",
    "for img_name in random_filenames[0:8]:\n",
    "        \n",
    "    with open(os.path.join (img_name), \"rb\") as image_contents:\n",
    "        results = predictor.classify_image(project.id, publish_iteration_name, image_contents.read())\n",
    "        \n",
    "        im = plt.imread(img_name)\n",
    "        grid[i].imshow(im,aspect='auto', extent=(0,1,0,0.5), alpha=1, origin='upper', zorder=-1), \n",
    "            \n",
    "        grid[i].text(0.05,0.05,\n",
    "                     results.predictions[0].tag_name +  \": {0:.2f}%\".format(results.predictions[0].probability * 100),\n",
    "                     fontsize=30,\n",
    "                     backgroundcolor='w')\n",
    "    \n",
    "    i = i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72425ce0",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Who is on the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/prime-minister.jpg\"\n",
    "pil_img = IPythonImage(image)\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(image), \"rb\") as image_stream:\n",
    "    detect_faces = computervision_client.analyze_image_in_stream(image_stream, [\"faces\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc877e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(image)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "# Overlay the information\n",
    "for face in detect_faces.faces:\n",
    "    color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "    rect = patches.Rectangle((face.face_rectangle.left, face.face_rectangle.top), \n",
    "                             face.face_rectangle.width, face.face_rectangle.height, \n",
    "                             linewidth=6, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.text(\n",
    "        (1/im.shape[1]*face.face_rectangle.left), 1-(1/im.shape[0]*face.face_rectangle.top), \n",
    "        \"{} of age {}\".format(face.gender,face.age),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='bottom',\n",
    "        fontsize=44,\n",
    "        color='w',\n",
    "        backgroundcolor=color,\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dede835",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(image), \"rb\") as image_stream:\n",
    "    detect_domain_results_celebs_remote = computervision_client.analyze_image_by_domain_in_stream(\"celebrities\",image_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(image)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "# Overlay the information\n",
    "for face in detect_domain_results_celebs_remote.result[\"celebrities\"]:\n",
    "    color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "    rect = patches.Rectangle((face['faceRectangle']['left'], face['faceRectangle']['top']), \n",
    "                             face['faceRectangle']['width'], face['faceRectangle']['height'], linewidth=6, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "   \n",
    "    ax.text(\n",
    "        (1/im.shape[1]*face['faceRectangle']['left']), 1-(1/im.shape[0]*face['faceRectangle']['top']), \n",
    "        \"{}\".format(face['name']),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='bottom',\n",
    "        fontsize=44,\n",
    "        color='w',\n",
    "        backgroundcolor=color,\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b20054",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get more insights on Faces\n",
    "\n",
    "[Face API](https://docs.microsoft.com/azure/cognitive-services/face/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef92a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_key = \"<INSERT KEY>\"\n",
    "face_api_endpoint = \"https://azconf-face.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad810844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated FaceClient.\n",
    "face_client = FaceClient(face_api_endpoint, CognitiveServicesCredentials(face_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_image = \"face-photos/modern_family.jpg\"\n",
    "display(IPythonImage(filename=mf_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66801a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    \n",
    "    # Detect faces in images\n",
    "    detected_faces = face_client.face.detect_with_stream(image_stream, return_face_attributes=[\n",
    "                    'age',  # Could have been the string 'age'\n",
    "                    'gender',\n",
    "                    'smile',\n",
    "                    'facialHair',\n",
    "                    'glasses',\n",
    "                    'emotion',\n",
    "                    'hair',\n",
    "                    'makeup',\n",
    "                    'accessories'\n",
    "                ])\n",
    "    \n",
    "    # Display the result\n",
    "    pil_img = Image.open(mf_image)\n",
    "    for face in detected_faces: \n",
    "        img2 = pil_img.crop((face.face_rectangle.left, face.face_rectangle.top, face.face_rectangle.left+face.face_rectangle.width, face.face_rectangle.top+face.face_rectangle.height))\n",
    "        display(img2)\n",
    "        print (f'Face id: {face.face_id}')\n",
    "        print (f'Gender: {face.face_attributes.gender}')\n",
    "        print (f'smile: {face.face_attributes.smile}')\n",
    "        print (f'age: {face.face_attributes.age}')\n",
    "        print (f'facial_hair moustache: {face.face_attributes.facial_hair.moustache}')\n",
    "        print (f'facial_hair beard: {face.face_attributes.facial_hair.beard}')\n",
    "        print (f'facial_hair sideburns: {face.face_attributes.facial_hair.sideburns}')\n",
    "        print (f'glasses: {face.face_attributes.glasses}')\n",
    "        print (f'eye_makeup: {face.face_attributes.makeup.eye_makeup}')\n",
    "        print (f'lip_makeup: {face.face_attributes.makeup.lip_makeup}')\n",
    "        print (f'emotion: {face.face_attributes.emotion}')\n",
    "        print(\" ==\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# Save this ID for use in Find Similar\n",
    "first_image_face_ID = detected_faces[0].face_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24188115",
   "metadata": {},
   "source": [
    "## Train the face API to recognize people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "path = r\"face-photos/train\"\n",
    "random_filenames = []\n",
    "for train_img in os.listdir(path):\n",
    "    random_filenames.append(os.path.join(path, train_img))\n",
    "\n",
    "grid = AxesGrid(plt.figure(1, (20,20)), 111, nrows_ncols=(1, 5), axes_pad=0, label_mode=\"1\")\n",
    "\n",
    "i = 0\n",
    "for img_name in random_filenames[0:10]:\n",
    "    im = plt.imread(img_name)\n",
    "    grid[i].imshow(im,aspect='auto', extent=(0,0.8,0,1), alpha=1, origin='upper', zorder=-1)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_GROUP_ID = \"modern-family\"\n",
    "face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=PERSON_GROUP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"face-photos/train\"\n",
    "\n",
    "for person in os.listdir(path):\n",
    "    name = person.partition(\".\")[0]\n",
    "    print(\"Adding:\"+name)\n",
    "    w = open(os.path.join(path,person), 'r+b')\n",
    "\n",
    "    # Create a person\n",
    "    person = face_client.person_group_person.create(PERSON_GROUP_ID, name)\n",
    "\n",
    "    # Add a face to the person\n",
    "    face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, person.person_id, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the person group\n",
    "face_client.person_group.train(PERSON_GROUP_ID)\n",
    "\n",
    "while (True):\n",
    "    training_status = face_client.person_group.get_training_status(PERSON_GROUP_ID)\n",
    "    print(\"Training status: {}.\".format(training_status.status))\n",
    "    if (training_status.status is TrainingStatusType.succeeded):\n",
    "        break\n",
    "    elif (training_status.status is TrainingStatusType.failed):\n",
    "        face_client.person_group.delete(person_group_id=PERSON_GROUP_ID)\n",
    "        sys.exit('Training the person group has failed.')\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7315576",
   "metadata": {},
   "source": [
    "## Identify people in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mf_image), \"rb\") as image_stream:\n",
    "    # Detect faces\n",
    "    face_ids = []\n",
    "    # We use detection model 3 to get better performance.\n",
    "    faces = face_client.face.detect_with_stream(image_stream, detection_model='detection_03')\n",
    "    for face in faces:\n",
    "        face_ids.append(face.face_id)\n",
    "        print(f'found face: {face.face_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44506a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify faces\n",
    "results = face_client.face.identify(face_ids, PERSON_GROUP_ID)\n",
    "\n",
    "identified_persons = {}\n",
    "\n",
    "for person in results:\n",
    "    for candidate in person.candidates:\n",
    "        identified_person = face_client.person_group_person.get(PERSON_GROUP_ID,candidate.person_id)\n",
    "        print(\"Found: \"+identified_person.name)\n",
    "        identified_persons[person.face_id] = identified_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "\n",
    "im = plt.imread(mf_image)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/70, im.shape[0]/70))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "# Overlay the information\n",
    "for face in faces:\n",
    "    color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "    rect = patches.Rectangle((face.face_rectangle.left, face.face_rectangle.top), \n",
    "                             face.face_rectangle.width, face.face_rectangle.height, \n",
    "                             linewidth=3, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.text(\n",
    "        (1/im.shape[1]*face.face_rectangle.left), 1-(1/im.shape[0]*face.face_rectangle.top), \n",
    "        \"{}\".format(identified_persons[face.face_id].name),\n",
    "        horizontalalignment='left', verticalalignment='bottom', fontsize=16, color='w', backgroundcolor=color, transform=ax.transAxes\n",
    "    )\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce94dc3",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Read text in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_image_url = \"images/satya-acc.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPythonImage(filename=read_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Start =====\")\n",
    "\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "with open(os.path.join(read_image_url), \"rb\") as image_stream:\n",
    "    read_response = computervision_client.read_in_stream(image_stream,  raw=True)\n",
    "\n",
    "    read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "    # Grab the ID from the URL\n",
    "    operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "    # Call the \"GET\" API and wait for it to retrieve the results \n",
    "    while True:\n",
    "        read_result = computervision_client.get_read_result(operation_id)\n",
    "        if read_result.status not in ['notStarted', 'running']:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        \n",
    "print(\"===== Done =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(read_image_url)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "for text_result in read_result.analyze_result.read_results:\n",
    "    for line in text_result.lines:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((line.bounding_box[0], line.bounding_box[1]), \n",
    "                             line.bounding_box[2]-line.bounding_box[0], line.bounding_box[7]-line.bounding_box[1], linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        print(line.text)\n",
    "    \n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee80930",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwriting_image_url = \"images/handwriting.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IPythonImage(filename=handwriting_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Start =====\")\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "with open(os.path.join(handwriting_image_url), \"rb\") as image_stream:\n",
    "    read_response = computervision_client.read_in_stream(image_stream,  raw=True)\n",
    "\n",
    "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results \n",
    "while True:\n",
    "    read_result = computervision_client.get_read_result(operation_id)\n",
    "    if read_result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"===== Done =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(handwriting_image_url)\n",
    "\n",
    "# Create figure and axes\n",
    "fig = plt.figure(figsize = (im.shape[1]/100, im.shape[0]/100))\n",
    "ax = plt.axes((0,0,1,1))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im,origin='upper')\n",
    "\n",
    "for text_result in read_result.analyze_result.read_results:\n",
    "    for line in text_result.lines:\n",
    "        color = (np.random.rand(),np.random.rand(),np.random.rand())\n",
    "        rect = patches.Rectangle((line.bounding_box[0], line.bounding_box[1]), \n",
    "                             line.bounding_box[2]-line.bounding_box[0], line.bounding_box[5]-line.bounding_box[1], \n",
    "                             linewidth=6, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        print(line.text)\n",
    "    \n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f018c",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "\n",
    "\n",
    "## Computer Vision\n",
    "- Describe images\n",
    "- Detect objects\n",
    "- Detect faces\n",
    "- Detrect celebrities \n",
    "\n",
    "## Custom Vision\n",
    "- Trained our own model to classify images\n",
    "\n",
    "## Face API\n",
    "- Describe faces in images\n",
    "- Trained our own person group\n",
    "- Identified persons in an image\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
